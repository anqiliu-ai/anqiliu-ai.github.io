[
  {
    "authors": null,
    "categories": null,
    "content": "\n\n",
    "date": 1656979200,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1656979200,
    "objectID": "bfc9941b6b6fd7b4ef09dd0ccd08af0c",
    "permalink": "https://anqiliu-ai.github.io/services/",
    "publishdate": "2022-07-05T00:00:00Z",
    "relpermalink": "/services/",
    "section": "",
    "summary": "",
    "tags": null,
    "title": "Services",
    "type": "page"
  },
  {
    "authors": null,
    "categories": null,
    "content": "\nSpring 2022: En.601.787 Machine Learning for Trustworthy AI. Syllabus\nFall 2022: En.601.675(475) Machine Learning.\nSpring 2023: En.601.787 Machine Learning for Trustworthy AI.\nSpring 2024: En.601.675(475) Machine Learning.\nFall 2024: En.601.787 Machine Learning for Trustworthy AI.\nSpring 2025: En.601.687(487) Machine Learning: Coping with Non-stationary Environments and Errors\n\n",
    "date": 1656979200,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1656979200,
    "objectID": "416f445d9bade50adf875443b512ceeb",
    "permalink": "https://anqiliu-ai.github.io/teaching/",
    "publishdate": "2022-07-05T00:00:00Z",
    "relpermalink": "/teaching/",
    "section": "",
    "summary": "Spring 2022: En.601.787 Machine Learning for Trustworthy AI. Syllabus\nFall 2022: En.601.675(475) Machine Learning.\nSpring 2023: En.601.787 Machine Learning for Trustworthy AI.\nSpring 2024: En.601.675(475) Machine Learning.\nFall 2024: En.601.787 Machine Learning for Trustworthy AI.\nSpring 2025: En.601.687(487) Machine Learning: Coping with Non-stationary Environments and Errors",
    "tags": null,
    "title": "Teaching",
    "type": "page"
  },
  {
    "authors": null,
    "categories": null,
    "content": "\nDistributionally Robust Learning (under data shifts):\nHaoxuan Wang, Anqi Liu, Zhiding Yu, Yisong Yue, and Anima Anandkumar. \u0026ldquo;Deep Distributionally Robust Learning for Calibrated Uncertainties under Domain Shift\u0026rdquo;, on Arxiv 2021.\nRizal Fathony, Kaiser Asif, Anqi Liu, Mohammad Ali Bashiri, Wei Xing, Sima Behpour, Xinhua Zhang, and Brian D. Ziebart \u0026ldquo;Consistent Robust Adversarial Prediction for General Multiclass Classification\u0026rdquo;, On Arxiv 2018.\nAnqi Liu and Brian D. Ziebart \u0026ldquo;Robust Covariate Shift Prediction with General Losses and Feature Views\u0026rdquo;, On Arxiv 2017.\nAnqi Liu, Rizal Fathony, and Brian D. Ziebart \u0026ldquo;Kernel Robust Bias-Aware Prediction under Covariate Shift\u0026rdquo;, On Arxiv.\nRizal Fathony, Anqi Liu, Kaiser Asif, and Brian D. Ziebart “Adversarial Multiclass Classification: A Risk Minimization Perspective”, In NeurIPS2016.\nXiangli Chen, Mathew Monfort, Anqi Liu, and Brian D. Ziebart “Robust Covariate Shift Regression”, In AISTATS2016.\nAnqi Liu and Brian D. Ziebart “Robust Classification under Sample Selection Bias”, In NeurIPS2014. Spotlight.\n Active Learning (under data shifts):\nEric Zhao, Anqi Liu, Anima Anandkumar, and Yisong Yue \u0026ldquo;Active Learning under Label Shift\u0026rdquo;, in AISTATS 2021.\nSima Behpour, Anqi Liu, and Brian D. Ziebart \u0026ldquo;Active Learning for Probabilistic Structured Prediction of Cuts and Matchings\u0026rdquo;, In ICML2019.\nKamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Anima Anandkumar \u0026ldquo;Regularized Learning for Domain Adaptation under Label Shifts\u0026rdquo;, In ICLR2019.\nAnqi Liu, Lev Reyzin, and Brian D. Ziebart “Shift-Pessimistic Active Learning using Robust Bias-Aware Prediction”, In AAAI2015.\n Safe Decision Making (under data shifts):\nHao Liu, Anima Anandkumar, Yisong Yue, and Anqi Liu. \u0026ldquo;Distributionally Robust Off-Policy Evaluation\u0026rdquo;, PDF coming soon.\nYashwanth Kumar Nakka, Anqi Liu, Guanya Shi, Anima Anandkumar, Yisong Yue, and Soon-Jo Chung. \u0026ldquo;Chance-Constrained Trajectory Optimization for Safe Exploration and Learning of Nonlinear Systems\u0026rdquo;, RA-L, 2020.\nAnqi Liu, Guanya Shi, Soon-Jo Chung, Anima Anandkumar, and Yisong Yue \u0026ldquo;Robust Regression for Safe Exploration in Control\u0026rdquo;, In L4DC 2020.\n Human Decision Making Modeling:\nAnqi Liu, Hao Liu, Tongxin Li, Saeed Karimi Bidhendi, Yisong Yue, and Anima Anandkumar \u0026ldquo;Disentangling Observed Causal Effects from Latent Confounders using Method of Moments\u0026rdquo;, on Arxiv 2021.\nQuanying Liu, Haiyan Wu, Anqi Liu \u0026ldquo;Modeling and Interpreting Real-world Human Risk Decision Making with Inverse Reinforcement Learning\u0026rdquo;, in Real-world Sequential Decision Making Workshop, ICML 2019.\nMathew Monfort, Anqi Liu, and Brian D. Ziebart “Trajectory Forecasting and Intent Recognition via Predictive Inverse Linear-Quadratic Regulation”, In AAAI2015.\n Fair Machine Learning:\nAshkan Rezaei, Anqi Liu, Omid Memarrast, and Brian D. Ziebart. \u0026ldquo;Robust Fairness Under Covariate Shift\u0026rdquo;, in AAAI 2021.\n Computational Social Science/AI for Social Good:\nMaya Srikanth, Anqi Liu, Nicholas Adams-Cohen, Jian Cao, R Michael Alvarez, Anima Anandkumar \u0026ldquo;Dynamic Social Media Monitoring for Fast-Evolving Online Discussions\u0026rdquo;, In KDD ADS track, 2021.\nAnqi Liu, Maya Srikanth, Nicholas Adams-Cohen, R Michael Alvarez, Anima Anandkumar \u0026ldquo;Finding Social Media Trolls: Dynamic Keyword Selection Methods for Rapidly-Evolving Online Debates\u0026rdquo;, In AI for Social Good workshop at NeurIPS, 2019.\n Text Mining/Information Retrieval:\nHong Wang, Anqi Liu, Jing Wang, Brian D Ziebart, Clement T Yu, Warren Shen \u0026ldquo;Context Retrieval for Web Tables\u0026rdquo;, In International Conference on The Theory of Information Retrieval, 2015\n\n",
    "date": 1631145600,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1631145600,
    "objectID": "480c4de99851329b51acecc000e2e84f",
    "permalink": "https://anqiliu-ai.github.io/publications/",
    "publishdate": "2021-09-09T00:00:00Z",
    "relpermalink": "/publications/",
    "section": "",
    "summary": "Distributionally Robust Learning (under data shifts):\nHaoxuan Wang, Anqi Liu, Zhiding Yu, Yisong Yue, and Anima Anandkumar. \u0026ldquo;Deep Distributionally Robust Learning for Calibrated Uncertainties under Domain Shift\u0026rdquo;, on Arxiv 2021.\nRizal Fathony, Kaiser Asif, Anqi Liu, Mohammad Ali Bashiri, Wei Xing, Sima Behpour, Xinhua Zhang, and Brian D. Ziebart \u0026ldquo;Consistent Robust Adversarial Prediction for General Multiclass Classification\u0026rdquo;, On Arxiv 2018.\nAnqi Liu and Brian D. Ziebart \u0026ldquo;Robust Covariate Shift Prediction with General Losses and Feature Views\u0026rdquo;, On Arxiv 2017.",
    "tags": null,
    "title": "A list of past publications (By area), not updated frequently. Please refer to google scholar for most updated info.",
    "type": "page"
  },
  {
    "authors": null,
    "categories": null,
    "content": "Current students:\nPhD students:\nManh-Ha Bui\nSally Cao (co-advised with Chien-Ming Huang)\nYihong Guo\nZhengping Jiang (co-advised with Ben Van Durme)\nAayush Mishra\nDrew Prinster (co-advised with Suchi Saria and Chien-Ming Huang)\nGina Wong (co-advised with Rama Chellappa)\nAndrea Wynn (co-advised with Eric Nalisnick)\n\nFor prospective students:\nI am always looking for motivated students (PhD, master, and undergraduate) to join my group. To start doing research, here are some great advice from my colleagues: For undergraduates and masters and How to be a successful PhD student. If you are interested in working with me, please feel free to fill this form. google form\n",
    "date": 1631145600,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1631145600,
    "objectID": "de354e522e3337061ca4d76bfdd32d47",
    "permalink": "https://anqiliu-ai.github.io/students/",
    "publishdate": "2021-09-09T00:00:00Z",
    "relpermalink": "/students/",
    "section": "",
    "summary": "Current students:\nPhD students:\nManh-Ha Bui\nSally Cao (co-advised with Chien-Ming Huang)\nYihong Guo\nZhengping Jiang (co-advised with Ben Van Durme)\nAayush Mishra\nDrew Prinster (co-advised with Suchi Saria and Chien-Ming Huang)\nGina Wong (co-advised with Rama Chellappa)\nAndrea Wynn (co-advised with Eric Nalisnick)\n\nFor prospective students:\nI am always looking for motivated students (PhD, master, and undergraduate) to join my group. To start doing research, here are some great advice from my colleagues: For undergraduates and masters and How to be a successful PhD student.",
    "tags": null,
    "title": "Students",
    "type": "page"
  },
  {
    "authors": null,
    "categories": null,
    "content": "This project covers a series of my work, ranging from fundamentals of distributionally robust learning under covariate shift, to its integration to real-world safe exploration and domain adaption tasks.\nDistributionally robust learning involves a minimax game between a predictor and an adversary, who is usually subject to constraints from data. Instead of focusing on the robustness against adversarial perturbations on the covariate variables, as in many recent literatures, I focus on using conditional output distributions as adversaries. This formulation has two major advantages: (1) It provides a conservative way to quantify the model uncertainties under covariate shift, which benefits data collection and experimental design on various real-world tasks. (2) It provides consistent predictors for minimizing non-smooth loss functions, which is usually elusive for the empirical risk minimization framework.\nHere are some featured papers in this line of work:\n In the vanilla supervised learning setting:\n The first paper using logloss in the framework under covariate shift: the robust bias-aware classifier: Anqi Liu and Brian D. Ziebart “Robust Classification under Sample Selection Bias”, In NeurIPS2014. Spotlight.\n The first paper minimizing non-smooth loss function in this framework that provides consistent analytical solutions for minimax games under constraints: Rizal Fathony, Anqi Liu, Kaiser Asif, and Brian D. Ziebart “Adversarial Multiclass Classification: A Risk Minimization Perspective”, In NeurIPS2016.\n The first paper solving regression problems in this framework under covariate shift that directly predicts Gaussian mean and variance for uncertainty estimation in continuous problems: Xiangli Chen, Mathew Monfort, Anqi Liu, and Brian D. Ziebart “Robust Covariate Shift Regression”, In AISTATS2016.\n The first paper providing unified solutions for the framework under the general loss settings, like for cost-sensitive and abstaining loss functions: Rizal Fathony, Kaiser Asif, Anqi Liu, Mohammad Ali Bashiri, Wei Xing, Sima Behpour, Xinhua Zhang, and Brian D. Ziebart \u0026ldquo;Consistent Robust Adversarial Prediction for General Multiclass Classification\u0026rdquo;, On Arxiv 2018.\n Generalizing the weighting in the original formulation to more flexible weighting functions: Jose I. Segovia-Martin, Santiago Mazuelas, Anqi Liu. \u0026ldquo;Double-Weighting for Covariate Shift Adaptation\u0026rdquo;, in ICML2023.\n  In the interactive learning setting:\n The first paper tackling the sample bias problems in active learning from a robust learning point of view: Anqi Liu, Lev Reyzin, and Brian D. Ziebart “Shift-Pessimistic Active Learning using Robust Bias-Aware Prediction”, In AAAI2015.\n The first paper integrating the conservative uncertainty quantification to help improve safe exploration efficiency and constraint satisfaction in real-world systems: Anqi Liu, Guanya Shi, Soon-Jo Chung, Anima Anandkumar, and Yisong Yue \u0026ldquo;Robust Regression for Safe Exploration in Control\u0026rdquo;, In L4DC 2020.\n The first paper providing robust dynamics estimation and end-to-end guarantees for safe planning in stochastic control systems: Yashwanth Kumar Nakka, Anqi Liu, Guanya Shi, Anima Anandkumar, Yisong Yue, and Soon-Jo Chung. \u0026ldquo;Chance-Constrained Trajectory Optimization for Safe Exploration and Learning of Nonlinear Systems\u0026rdquo;, RA-L, 2020.\n  In the fair learning setting:\n The first paper dealing with the intersection between covariate shift and fairness from a robust learning point of view: Ashkan Rezaei, Anqi Liu, Omid Memarrast, and Brian D. Ziebart. \u0026ldquo;Robust Fairness Under Covariate Shift\u0026rdquo;, AAAI2021.  In the large-scale deep learning setting:\n The first paper scaling up the distributionally robust learning under covariate shift to solve large scale sim-to-real unsupervised domain adaptation tasks: Haoxuan Wang, Zhiding Yu, Yisong Yue, and Anima Anandkumar, Anqi Liu, Junchi Yan. \u0026ldquo;Learning Calibrated Uncertainties for Domain Shift: A Distributionally Robust Learning Approach\u0026rdquo;, in IJCAI 2023.\n Motivating calibrated uncertainty estimation for deep classification under distribution shifts: \u0026ldquo;Density-Softmax: Scalable and Calibrated Uncertainty Estimation under Distribution Shifts\u0026rdquo;, on ArXiv 2023.\n Motivating calibrated uncertainty estimation for deep regression under distribution shifts: Ha Manh Bui, Anqi Liu. \u0026ldquo;Density-Regression: Efficient and Distance-Aware Deep Regressor for Uncertainty Estimation under Distribution Shifts\u0026rdquo;, in AISTATS2024\n  In offline policy evaluation and learning setting in contextual bandits:\n The first paper taking a covariate shift regression point of view to deal with offline policy learning in contextual bandits: Zhouhao Yang, Yihong Guo, Pan Xu, Anqi Liu, Animashree Anandkumar. \u0026ldquo;Distributionally Robust Policy Gradient for Offline Contextual Bandits\u0026rdquo;, in AISTATS2023.   ",
    "date": -62135596800,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": -62135596800,
    "objectID": "8b5b65047e904440b04afb4d43e8cac8",
    "permalink": "https://anqiliu-ai.github.io/project/drl/",
    "publishdate": "0001-01-01T00:00:00Z",
    "relpermalink": "/project/drl/",
    "section": "project",
    "summary": "This project covers a series of my work, ranging from fundamentals of  distributionally robust learning under covariate shift, to its integration to real-world safe exploration and domain adaption tasks. Media Coverage: [The Value of Saying ‘I Don’t Know’](https://engineering.uic.edu/about/coe-news/rise/rise/the-value-of-saying-i-dont-know/).",
    "tags": null,
    "title": "Distributionally Robust Learning under Covariate Shift",
    "type": "project"
  },
  {
    "authors": null,
    "categories": null,
    "content": "This project covers a series of my work on uncertainty quantification under distribution shift, especially covariate shift. We look at various metrics to evaluate the quality of uncertainty estimates, ranging from marginal coverage, sharpness, to expected calibration errors (for subgroups). We aim to develop rigorous and principled methods that is also practical in real world applications. For example, we try to improve and better balance the sample complexity and computational complexity for popular distributon-free methods like conformal predictions. We also utilize data beyond traditional labeled data in supervised learning for the purpose of model calibration. For example, we leverage human annotation to \u0026ldquo;align\u0026rdquo; the LLM better with human uncertainty.\nHere are some featured papers in this line of work:\n A series of wrapper methods for distribution-free uncertainty quantification tasks under covariate shift, utilizing the more sample efficient jackknife+ methods: Andrew Prinster, Anqi Liu, Suchi Saria. \u0026ldquo;JAWS: Auditing Predictive Uncertainty Under Covariate Shift\u0026rdquo;, in NeurIPS 2022.\n A collection of methods based on the jackknife+ that achieve a practical balance of computational and statistical efficiency: Drew Prinster, Suchi Saria, Anqi Liu. \u0026ldquo;JAWS-X: Addressing Efficiency Bottlenecks of Conformal Prediction Under Standard and Feedback Covariate Shift\u0026rdquo;, in ICML 2023.\n Calibrated and efficient uncertainty estimation for deep classification under distribution shifts: \u0026ldquo;Density-Softmax: Scalable and Calibrated Uncertainty Estimation under Distribution Shifts\u0026rdquo;, on ArXiv 2023.\n Calibrated and efficient uncertainty estimation for deep regression under distribution shifts: Ha Manh Bui, Anqi Liu. \u0026ldquo;Density-Regression: Efficient and Distance-Aware Deep Regressor for Uncertainty Estimation under Distribution Shifts\u0026rdquo;, in AISTATS2024\n Calibrating pre-trained large language models in cross-lingual tasks: Zhengping Jiang, Anqi Liu, Benjamin Van Durme. \u0026ldquo;Calibrating Zero-shot Cross-lingual (Un-)structured Predictions\u0026rdquo;, in EMNLP2022.\n Human scalar annotation for better calibrating large language models: Zhengping Jiang, Anqi Liu, Benjamnin Van Durme. Addressing the Binning Problem in Calibration Assessment through Scalar Annotations, in TACL 2024.\n  : * PIs: [Mike Alvarez (Caltech)](https://www.hss.caltech.edu/people/r-m-michael-alvarez), [Anima Anandkumar (Caltech)](http://tensorlab.cms.caltech.edu/users/anima/), [Anqi Liu (JHU)](https://anqiliu-ai.github.io/) * Postdocs: [Jian Cao](https://www.hss.caltech.edu/people/jian-cao), [Rafał Kocielnik](https://rkocielnik.com/) * Graduate Students: [Danny Ebanks](https://www.linkedin.com/in/daniel-ebanks-9b89a298), [Jacob Morrier](https://www.hss.caltech.edu/people/jacob-morrier), [Zhuofang Li](https://www.hss.caltech.edu/people/zhuofang-li), [Claudia Kann](https://www.hss.caltech.edu/people/claudia-kann), [Md. Yasin Kabir](https://scholar.google.com/citations?user=nm9qB7kAAAAJ\u0026hl=en) * Undergraduate Students: [Sara Kangaslahti](https://www.linkedin.com/in/sara-kangaslahti), [Sarah Hashash](https://www.linkedin.com/in/sarah-hashash-65603a1a7) * Industry Collaborators: [Jean Kossaifi](http://jeankossaifi.com/), [Bojan Tunguz](https://www.linkedin.com/in/tunguz) * Past students and postdocs: * [Nicholas Adams-Cohen](https://www.nadamscohen.com/), Now researcher at Accenture * [Maya Srikanth](https://www.linkedin.com/in/maya-s-srikanth), Now graduate student at Stanford * [Shunto J. Kobayashi](https://www.linkedin.com/in/shunto-kobayashi-323015113) * [Shiyu Zhang](https://www.linkedin.com/in/shiyu-zhang-1783608b) -- ",
    "date": -62135596800,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": -62135596800,
    "objectID": "e38e9e1fe5b538cfacdd7de7fbfbd5b2",
    "permalink": "https://anqiliu-ai.github.io/project/social/",
    "publishdate": "0001-01-01T00:00:00Z",
    "relpermalink": "/project/social/",
    "section": "project",
    "summary": "We aim to tackle two key challenges in model auditing for safeguarding AI. The first is the ubiquitous distribution shift, especially subpopulation shift. The second is that many UQ approaches require either intensive computing power or an impractical amount or quality of data that may be unavailable in real-world scenarios. Media Coverage: [Putting trust to the test](https://www.cs.jhu.edu/news/putting-trust-to-the-test/).",
    "tags": null,
    "title": "UQ for AI Safety and Fairness",
    "type": "project"
  }
]